{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc946e6a-63b5-4d55-8ee6-1086e563b5f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Welcome to the Databricks GenAI Workshop - Part Two: Retrieval Augmented Generation\n",
    "In this section, we will explore how to augment models by using RAG (Retrieval Augmented Generation). You'll gain hands-on experience with:\n",
    "\n",
    "- **Concepts for Pre-processing Data:** Briefly cover how to set up tables in a medallion architecture and set your data up for RAG.\n",
    "- **Creating a Vector Search Index:** Learn how to take a Delta table and use Databricks Vector Search and an embedding model to easily create a Vector Search index.\n",
    "- **Understanding Similarity Search:** Explore how to retrieve relevant data using similarity search against a vector index.\n",
    "- **Chaining Relevant Data into our Model:** See an example of using prompts and vector search to bring in relevant context and improve our model's response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15649cd-4a42-4f80-a03a-2f6be6e2bbde",
     "showTitle": true,
     "title": "Install the needed libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade databricks-vectorsearch langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484f2eb1-8f57-4ffe-86a7-41485e9f87a0",
     "showTitle": true,
     "title": "Restart Python Kernel"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c07e70-b376-4910-8c4e-6845385484ed",
     "showTitle": true,
     "title": "Setup User-Specific Variables"
    }
   },
   "outputs": [],
   "source": [
    "#setup catalog and show widget at top\n",
    "dbutils.widgets.text(\"catalog_name\",\"main\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "\n",
    "#break user in their own schema\n",
    "current_user = spark.sql(\"SELECT current_user() as username\").collect()[0].username\n",
    "schema_name = f'genai_workshop_{current_user.split(\"@\")[0].split(\".\")[0]}'\n",
    "\n",
    "#create schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "print(f\"\\nUsing catalog + schema: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fee974f-f4b7-4da7-92df-c0333f1f3001",
     "showTitle": true,
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from operator import itemgetter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d017444d-706e-4d66-8e18-0b8aa79edd04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### We've parsed out a couple research papers authored by Matei Zaharia (and colleagues) that represent data outside of the knowledge of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffa7307-a500-4982-9393-0cd4d1495ad8",
     "showTitle": true,
     "title": "Lets look at the parsed data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select parsed_output.text from main.default.silver_pdf_landing_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "697b1c69-e86a-405d-a593-276acf23d8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### This data was then broken down into chunks in preperation to be turned into vectors\n",
    "\n",
    "#### These chunks are going to then be turned into vectors through the use of Databricks Vector Store and a Databricks hosted embedding model\n",
    "\n",
    "![Example Flow for Vector Store](https://docs.databricks.com/en/_images/calculate-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194c3f14-c156-4e39-9bbd-c671246dcf50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### We've created a Vector Search Index prepared with this sample data - lets see how we can connect to it and use it to retrieve relevant contextual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d687145-cab2-4347-ace4-6ddb93a3aa7c",
     "showTitle": true,
     "title": "Connect to Vector Search"
    }
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Connect to the Vector Search Index\n",
    "############\n",
    "\n",
    "#Grab our Databricks Personal Access Token \n",
    "token = dbutils.secrets.get(scope = \"vs_endpoint\", key = \"databricks_token\")\n",
    "\n",
    "#Create client with proper credentials\n",
    "vs_client = VectorSearchClient(disable_notice=True, personal_access_token=token)\n",
    "vs_index = vs_client.get_index(\n",
    "    endpoint_name=\"vs_genai_lab\",\n",
    "    index_name=\"main.default.gold_pdf_landing_chunked_index\",\n",
    ")\n",
    "\n",
    "#provide the schema for underlying table\n",
    "vector_search_schema = {\n",
    "    \"primary_key\": \"chunk_id\",\n",
    "    \"chunk_text\": \"chunked_text\",\n",
    "    \"document_source\": \"doc_uri\"\n",
    "}\n",
    "\n",
    "#Create a retriever from our Vector Search Index\n",
    "# k value defines how many top results we'd like to return\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    columns=[\n",
    "        vector_search_schema.get(\"primary_key\"),\n",
    "        vector_search_schema.get(\"chunk_text\"),\n",
    "        vector_search_schema.get(\"document_source\"),\n",
    "    ],\n",
    ").as_retriever(search_kwargs={\"k\":3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4060a0c-b8ad-4dca-aca5-94da5d456181",
     "showTitle": true,
     "title": "Test Vector Search Retrieval"
    }
   },
   "outputs": [],
   "source": [
    "#Lets test the retrieval from our vector search endpoint by asking it about specific data within our research papers\n",
    "\n",
    "results = vs_index.similarity_search(\n",
    "    query_text=\"What is ARES?\",\n",
    "    columns=[\"chunk_id\", \"chunked_text\"],\n",
    "    num_results=2\n",
    "    )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712699f8-8e88-4b0a-a470-3d13929ab461",
     "showTitle": true,
     "title": "Test our model for knowledge"
    }
   },
   "outputs": [],
   "source": [
    "#In part 1 of this workshop, we asked our DBRX model what ARES was to test our hallucination reducing prompt\n",
    "\n",
    "model = ChatDatabricks(\n",
    "    endpoint=\"databricks-dbrx-instruct\",\n",
    "    max_tokens = 400,\n",
    ")\n",
    "\n",
    "#Lets remind ourselves that it does not know what ARES is - this time without the anti-hallicunation prompt\n",
    "#Note: ARES is NOT the Atmospheric Remote-Sensing Infrared Exoplanet Large-survey - this is a hallucination\n",
    "print(model.invoke(\"What is ARES?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad0f8b19-2d6d-4222-9ee3-a70d770b25b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Now lets tie our model and retriever together, along with some prompts needed as glue. Here's a breakdown of how this looks in action.\n",
    "\n",
    "![Example Flow for RAG](https://docs.databricks.com/en/_images/rag-workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e735b58f-66e4-4e0a-93aa-c7644f9bdac9",
     "showTitle": true,
     "title": "RAG Chain Creation"
    }
   },
   "outputs": [],
   "source": [
    "#Helper function to take retrieved docs and join them together\n",
    "def format_context(docs):\n",
    "    chunk_template = \"`{chunk_text}`\\n\"\n",
    "    chunk_contents = [chunk_template.format(chunk_text=d.page_content) for d in docs]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "#First prompt to bring in context and set the model's behavior\n",
    "prompt_template = \"You are a trusted assistant that helps answer questions about academic research based only on the provided information. If you do not know the answer to a question, you truthfully say you do not know.  Here is some context which might or might not help you answer: {context}.  Answer directly, do not repeat the question, do not start with something like: the answer to the question, do not add AI in front of your answer, do not say: here is the answer, do not mention the context or the question. Based on this history and context, answer this question: {question}.\"\n",
    "\n",
    "#Define the variables for first prompt\n",
    "chat_prompt_template_variables=[\"context\",\"question\"]\n",
    "\n",
    "#Similar to part 1 - we're creating prompt templates\n",
    "prompt = PromptTemplate(\n",
    "    template= prompt_template,\n",
    "    input_variables=chat_prompt_template_variables,\n",
    ")\n",
    "\n",
    "#Second prompt used to write a query for the retrieved context based on the user's question\n",
    "query_rewriter_prompt_template = \"Based on the chat history below, we want you to generate a query for an external data source to retrieve relevant documents so that we can better answer the question. The query should be in natual language. The external data source uses similarity search to search for relevant documents in a vector space. So the query should be similar to the relevant documents semantically. Answer with only the query. Do not add explanation. Question: {question}\"\n",
    "query_rewriter_prompt_template_variables= [\"question\"]\n",
    "\n",
    "#Similar to part 1 - we're creating prompt templates\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    template=query_rewriter_prompt_template,\n",
    "    input_variables=query_rewriter_prompt_template_variables,\n",
    ")\n",
    "\n",
    "#Tie them together to create a RAG chain. Components are chained together in order they appear\n",
    "rag_chain = (\n",
    "    {\n",
    "            \"context\": query_rewrite_prompt     #assemble rewriter prompt to add user's question\n",
    "            | model                             #define the model to be used\n",
    "            | StrOutputParser()                 #parses the string output generated by the model\n",
    "            | vector_search_as_retriever        #performs similarity search and returns context\n",
    "            | RunnableLambda(format_context),   #formats the returned results\n",
    "            \"question\": itemgetter(\"question\"), #adds the original user's question\n",
    "        }\n",
    "    | prompt                                    #main prompt that now has both the context variable as well as question\n",
    "    | model                                     #model to be used\n",
    "    | StrOutputParser()                         #final output\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7baa75d0-5774-4896-9216-3f76eca8ae07",
     "showTitle": true,
     "title": "Run RAG Chain"
    }
   },
   "outputs": [],
   "source": [
    "#Create out sample prompt - lets set up our question from earlier and see if it's learned\n",
    "input_sample = {\n",
    "    \"question\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is ARES?\",\n",
    "        }]}\n",
    "        \n",
    "#Run the chain!\n",
    "print(rag_chain.invoke(input_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf70cbf-21ef-47d2-90f3-1bad4923cdfb",
     "showTitle": true,
     "title": "Create MLflow Experiment for RAG Chain"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import cloudpickle\n",
    "import langchain\n",
    "\n",
    "# Create a new mlflow experiment or get the existing one if already exists.\n",
    "current_user = spark.sql(\"SELECT current_user() as username\").collect()[0].username\n",
    "experiment_name = f\"/Users/{current_user}/genai-prompt-engineering-workshop\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# set the name of our model\n",
    "model_name = \"dbrx_chain_rag\"\n",
    "\n",
    "# Define the function to return a retriever\n",
    "def loader_fn():\n",
    "    return vector_search_as_retriever()\n",
    "\n",
    "# get experiment id to pass to the run\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "with mlflow.start_run(experiment_id=experiment_id):\n",
    "    mlflow.langchain.log_model(\n",
    "        rag_chain,\n",
    "        model_name,\n",
    "        loader_fn = loader_fn,\n",
    "        input_example=input_sample,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "            \"pydantic==2.5.2 --no-binary pydantic\",\n",
    "            \"cloudpickle==\" + cloudpickle.__version__\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd10ae22-e59a-4498-a5af-f3d441b23e82",
     "showTitle": true,
     "title": "Register our Model in Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#grab our most recent run (which logged the model) using our experiment ID\n",
    "runs = mlflow.search_runs([experiment_id])\n",
    "last_run_id = runs.sort_values('start_time', ascending=False).iloc[0].run_id\n",
    "\n",
    "#grab the model URI that's generated from the run\n",
    "model_uri = f\"runs:/{last_run_id}/{model_name}\"\n",
    "\n",
    "#log the model to catalog.schema.model. The schema name referenced below is generated for you in the init script\n",
    "catalog = dbutils.widgets.get(\"catalog_name\")\n",
    "schema = schema_name\n",
    "\n",
    "#set our registry location to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=f\"{catalog}.{schema}.{model_name}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 755348635689087,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GenAI Workshop Part 2 - RAG",
   "widgets": {
    "catalog_name": {
     "currentValue": "main",
     "nuid": "0441aef0-afdc-450b-af9c-23a3538530e0",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
